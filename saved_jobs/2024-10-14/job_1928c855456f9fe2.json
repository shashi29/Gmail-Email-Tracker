[
    {
        "company": "Tanisha Systems Inc",
        "job_title": "AWS Pyspark with Python",
        "location": "Plano, TX, USA",
        "full_location": {
            "city": "Plano",
            "state": "TX",
            "country": "USA"
        },
        "job_details": {
            "employment_type": [
                "contract",
                "full-time",
                "third party"
            ],
            "experience_required": "12+ Years",
            "degree_required": "Bachelor\u2019s Degree in Computer Science/Programming or similar is preferred",
            "visa_sponsorship": "Must have legal right to work in the USA",
            "notice_period": "Not specified",
            "duration": "Not specified",
            "rate": "Not specified"
        },
        "skills": {
            "core": [
                "Python",
                "SQL",
                "Spark",
                "Data Engineering",
                "Communication Skills"
            ],
            "primary": [
                "AWS",
                "Relational Datastores",
                "NoSQL Datastores",
                "Cloud Object Stores",
                "Data Processing Infrastructure"
            ],
            "secondary": [
                "Terraform",
                "Cloud Certification",
                "Hudi",
                "Iceberg",
                "Delta Lake",
                "Data Observability Solutions",
                "Data Governance Frameworks",
                "Java",
                "Go",
                "DevOps Tools",
                "Infrastructure-as-Code",
                "Environment-as-Code",
                "Apache Kafka",
                "Logging Tools",
                "Monitoring Tools",
                "Shell Scripts",
                "Gradle",
                "Maven",
                "Jenkins",
                "Spinnaker",
                "Microservices Architecture"
            ],
            "all": [
                "Python",
                "SQL",
                "Spark",
                "Data Engineering",
                "Communication Skills",
                "AWS",
                "Relational Datastores",
                "NoSQL Datastores",
                "Cloud Object Stores",
                "Data Processing Infrastructure",
                "Terraform",
                "Cloud Certification",
                "Hudi",
                "Iceberg",
                "Delta Lake",
                "Data Observability Solutions",
                "Data Governance Frameworks",
                "Java",
                "Go",
                "DevOps Tools",
                "Infrastructure-as-Code",
                "Environment-as-Code",
                "Apache Kafka",
                "Logging Tools",
                "Monitoring Tools",
                "Shell Scripts",
                "Gradle",
                "Maven",
                "Jenkins",
                "Spinnaker",
                "Microservices Architecture"
            ]
        },
        "job_type": [
            "onsite"
        ],
        "contact_person": "Rishav Verma",
        "email": "rishav.verma@tanishasystems.com",
        "jd": "Greetings, My name is Rishav and I'm an IT recruiter at Tanisha Systems Our records show that you are an experienced IT professional with experience in AWS Pyspark with Python. This experience is relevant to one of my current openings. The opening requires good communication skills in addition to the above skills. It is in Plano TX / Wilmington DE (5 Days Onsite \u2013 No Remote) Job Type :: Contract/Fulltime Job Title:- AWS Pyspark with Python Job Location:- Plano TX / Wilmington DE (5 Days Onsite \u2013 No Remote) Experience: 12+ Years & Locals Only Note: In-person Interview is mandatory Job Description: Duties and responsibilities Mandatory Skills: 5+ years of experience in a data engineering position Proficiency is Python (or similar) and SQL Strong experience building data pipelines with Spark Strong verbal & written communication Strong analytical and problem solving skills Experience with relational datastores, NoSQL datastores and cloud object stores Experience building data processing infrastructure in AWS Bonus: Experience with infrastructure as code solutions, preferably Terraform Bonus: Cloud certification Bonus: Production experience with ACID compliant formats such as Hudi, Iceberg or Delta Lake Bonus: Familiar with data observability solutions, data governance frameworks Requirements Bachelor\u2019s Degree in Computer Science/Programming or similar is preferred Right to work Must have legal right to work in the USA Job responsibilities Required qualifications, capabilities, and skills Formal training or certification on software engineering concepts and 10+ years applied experience Hands-on practical experience delivering system design, application development, testing, and operational stability Advanced in one or more programming language(s) - Java, Python, Go A strong understanding of business technology drivers and their impact on architecture design, performance and monitoring, best practices Design and building web environments on AWS, which includes working with services like EC2, ALB, NLB, Aurora Postgres, DynamoDB, EKS, ECS fargate, MFTS, SQS/SNS, S3 and Route53 Advanced in modern technologies such as: Java version 8+, Spring Boot, Restful Microservices, AWS or Cloud Foundry, Kubernetes. Experience using DevOps tools in a cloud environment, such as Ansible, Artifactory, Docker, GitHub, Jenkins, Kubernetes, Maven, and Sonar Qube Experience and knowledge of writing Infrastructure-as-Code (IaC) and Environment-as-Code (EaC), using tools like CloudFormation or Terraform Experience with high volume, SLA critical applications, and building upon messaging and or event-driven architectures Deep understanding of financial industry and their IT systems Preferred qualifications, capabilities, and skills Expert in one or more programming language(s) preferably Java AWS Associate level certification in Developer, Solutions Architect or DevOps Experience in building the AWS infrastructure like EKS, EC2, ECS, S3, DynamoDB, RDS, MFTS, Route53, ALB, NLB Experience with high volume, mission critical applications, and building upon messaging and or event-driven architectures using Apache Kafka Experience with logging, observability and monitoring tools including Splunk, Datadog, Dynatrace. CloudWatch or Grafana Experience in automation and continuous delivery methods using Shell scripts, Gradle, Maven, Jenkins, Spinnaker Experience with microservices architecture, high volume, SLA critical applications and their interdependencies with other applications, microservices and databases Experience developing process, tooling, and methods to help improve operational maturity Thanks & Regards Rishav Verma Sr. Technical Recruiter Tanisha Systems Inc. [99 Wood Ave South Suite # 308, Iselin, NJ 08830] Office Number: 732-490-4608 | Ext: 429 Email: rishav.verma@tanishasystems.com LinkedIn: https://www.linkedin.com/in/rishav-verma-93783b172/ Web: www.tanishasystems.com",
        "source": "Email",
        "date_posted": "2024-10-14 13:57:44-05:00",
        "unique_id": "1928c855456f9fe2",
        "emp_type": [
            "contract",
            "full-time",
            "third party"
        ],
        "tag": "AWS Data Engineer"
    }
]