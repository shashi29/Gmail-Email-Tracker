[
    {
        "company": "Tanisha Systems Inc",
        "job_title": "AWS PySpark With Python",
        "location": "Plano, TX, USA",
        "full_location": {
            "city": "Plano",
            "state": "TX",
            "country": "USA"
        },
        "job_details": {
            "employment_type": [
                "contract",
                "third party"
            ],
            "experience_required": "8+ years",
            "degree_required": "Not specified",
            "visa_sponsorship": "Not specified",
            "notice_period": "Not specified",
            "duration": "Not specified",
            "rate": "Not specified"
        },
        "skills": {
            "core": [
                "AWS",
                "Python",
                "Java",
                "SQL",
                "Docker",
                "Kubernetes"
            ],
            "primary": [
                "Cloud Migration",
                "System Design",
                "Application Development",
                "DevOps",
                "Infrastructure-as-Code"
            ],
            "secondary": [
                "Go",
                "Node",
                "Scala",
                "Ansible",
                "GitHub",
                "Jenkins",
                "Maven",
                "Sonar Qube",
                "Apache Kafka",
                "Splunk",
                "Datadog",
                "Dynatrace",
                "Gradle",
                "Spinnaker",
                "Microservices Architecture"
            ],
            "all": [
                "AWS",
                "Python",
                "Java",
                "SQL",
                "Docker",
                "Kubernetes",
                "Cloud Migration",
                "System Design",
                "Application Development",
                "DevOps",
                "Infrastructure-as-Code",
                "Go",
                "Node",
                "Scala",
                "Ansible",
                "GitHub",
                "Jenkins",
                "Maven",
                "Sonar Qube",
                "Apache Kafka",
                "Splunk",
                "Datadog",
                "Dynatrace",
                "Gradle",
                "Spinnaker",
                "Microservices Architecture"
            ]
        },
        "job_type": [
            "onsite"
        ],
        "contact_person": "Kajal Vashishth",
        "email": "kajal.vashishth@tanishasystems.com",
        "jd": "Greetings, My name is Kajal, and I am Sr. Technical Recruiter at. Tanisha Systems Inc is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients. Role: AWS PySpark With PythonLocation : Plano TX / Wilmington DE (Onsite- 3 days per week)Only LocalType \u2013 Contract Experience: 8+Years Job Description:Job responsibilities:\u2022 Your experience in public cloud migrations of complex systems, anticipating problems, and finding ways to mitigate risk, will be key in leading numerous public cloud initiatives\u2022 Executes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\u2022 Own end-to-end platform issues & help provide solutions to platform build and performance issues on the AWS Cloud & ensure the deliverables are bug freeDrive, support, and deliver on a strategy to build broad use of Amazon's utility computing web services (e.g., AWS EC2, AWS S3, AWS RDS, AWS CloudFront, AWS EFS, AWS DynamoDB, CloudWatch, EKS, ECS, MFTS, ALB, NLB)\u2022 Design resilient, secure, and high performing platforms in Public Cloud using best practices\u2022 Measure and optimize system performance, with an eye toward pushing our capabilities forward, getting ahead of customer needs, and innovating to continually improve\u2022 Provide primary operational support and engineering for the public cloud platform and debug and optimize systems and automate routine tasks\u2022 Collaborate with a cross-functional team to develop real-world solutions and positive user experiences at every interaction\u2022 Drive Game days, Resiliency tests and Chaos engineering exercises\u2022 Utilize programming languages like Java, Python, SQL, Node, Go, and Scala, Open-Source RDBMS and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of AWS tools and services Required qualifications, capabilities, and skills:\u2022 Formal training or certification on software engineering concepts and 10+ years applied experience\u2022 Hands-on practical experience delivering system design, application development, testing, and operational stability\u2022 Advanced in one or more programming language(s) - Java, Python, Go\u2022 A strong understanding of business technology drivers and their impact on architecture design, performance and monitoring, best practices\u2022 Design and building web environments on AWS, which includes working with services like EC2, ALB, NLB, Aurora Postgres, DynamoDB, EKS, ECS fargate, MFTS, SQS/SNS, S3 and Route53\u2022 Advanced in modern technologies such as: Java version 8+, Spring Boot, Restful Microservices, AWS or Cloud Foundry, Kubernetes.\u2022 Experience using DevOps tools in a cloud environment, such as Ansible, Artifactory, Docker, GitHub, Jenkins, Kubernetes, Maven, and Sonar Qube\u2022 Experience and knowledge of writing Infrastructure-as-Code (IaC) and Environment-as-Code (EaC), using tools like CloudFormation or Terraform\u2022 Experience with high volume, SLA critical applications, and building upon messaging and or event-driven architectures\u2022 Deep understanding of financial industry and their IT systems Preferred qualifications, capabilities, and skills:\u2022 Expert in one or more programming language(s) preferably Java\u2022 AWS Associate level certification in Developer, Solutions Architect or DevOps\u2022 Experience in building the AWS infrastructure like EKS, EC2, ECS, S3, DynamoDB, RDS, MFTS, Route53, ALB, NLB\u2022 Experience with high volume, mission critical applications, and building upon messaging and or event-driven architectures using Apache Kafka\u2022 Experience with logging, observability and monitoring tools including Splunk, Datadog, Dynatrace. CloudWatch or Grafana\u2022 Experience in automation and continuous delivery methods using Shell scripts, Gradle, Maven, Jenkins, Spinnaker\u2022 Experience with microservices architecture, high volume, SLA critical applications and their interdependencies with other applications, microservices and databases\u2022 Experience developing process, tooling, and methods to help improve operational maturity",
        "source": "Email",
        "date_posted": "2024-10-14 16:57:33-05:00",
        "unique_id": "1928d29f4748d9fa",
        "emp_type": [
            "contract",
            "third party"
        ],
        "tag": "AWS Data Engineer"
    }
]