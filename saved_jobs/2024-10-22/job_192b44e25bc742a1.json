[
    {
        "company": "CAGUS",
        "job_title": "AWS Data Engineer",
        "location": "Plano, TX, USA",
        "full_location": {
            "city": "Plano",
            "state": "TX",
            "country": "USA"
        },
        "job_details": {
            "employment_type": [
                "third party"
            ],
            "job_code": "",
            "experience_required": "Not specified",
            "degree_required": "Not specified",
            "visa_sponsorship": "Not specified",
            "notice_period": "Not specified",
            "duration": "Not specified",
            "rate": "Not specified"
        },
        "skills": {
            "core": [
                "PySpark",
                "Java",
                "SQL",
                "AWS"
            ],
            "primary": [
                "Snowflake",
                "Kafka",
                "Kinesis"
            ],
            "secondary": [
                "AWS Step Functions",
                "AWS Lambda",
                "ETL",
                "ELT",
                "Problem-solving",
                "Communication"
            ],
            "all": [
                "PySpark",
                "Java",
                "SQL",
                "AWS",
                "Snowflake",
                "Kafka",
                "Kinesis",
                "AWS Step Functions",
                "AWS Lambda",
                "ETL",
                "ELT",
                "Problem-solving",
                "Communication"
            ]
        },
        "job_type": [],
        "contact_person": "Fizal",
        "email": "fizal.a@cagus.com",
        "jd": "From: Fizal, CAGUS fizal.a@cagus.com Reply to: fizal.a@cagus.com Position : AWS Data Engineer Location : Plano, Tx. Skill: Pyspark, Snowflake, Java, AWS Key Responsibilities: Develop and maintain PySpark-based data pipelines for processing large datasets. Implement event-driven architectures (Kafka, Kinesis) for real-time data processing. Design and optimize data pipelines in Snowflake. Automate workflows using AWS Step Functions, Lambda, and other cloud services. Integrate data from various sources and optimize ETL/ELT processes. Collaborate with cross-functional teams to deliver actionable data insights. Skills & Qualifications: Strong experience with PySpark, Java, and SQL. Proficiency in AWS services (S3, Lambda, Kinesis, Glue, Step Functions). Expertise in Snowflake for data warehousing. Experience in event-driven architecture and real-time data processing. Strong problem-solving and communication skills.",
        "source": "Email",
        "date_posted": "2024-10-22 07:22:36-05:00",
        "unique_id": "192b44e25bc742a1",
        "emp_type": [
            "third party"
        ],
        "tag": "AWS Data Engineer"
    }
]