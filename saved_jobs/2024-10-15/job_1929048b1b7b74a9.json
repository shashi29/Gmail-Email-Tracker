[
    {
        "company": "Tanisha Systems Inc",
        "job_title": "AWS PySpark With Python",
        "location": "Plano, TX, USA",
        "full_location": {
            "city": "Plano",
            "state": "TX",
            "country": "USA"
        },
        "job_details": {
            "employment_type": [
                "contract",
                "third party"
            ],
            "experience_required": "10+ years",
            "degree_required": "Not specified",
            "visa_sponsorship": "Not specified",
            "notice_period": "Not specified",
            "duration": "Not specified",
            "rate": "Not specified"
        },
        "skills": {
            "core": [
                "AWS",
                "Python",
                "Java",
                "SQL",
                "Docker",
                "Kubernetes"
            ],
            "primary": [
                "Go",
                "Scala",
                "CloudFormation",
                "Terraform",
                "Ansible",
                "GitHub",
                "Jenkins",
                "Maven",
                "Sonar Qube"
            ],
            "secondary": [
                "AWS EC2",
                "AWS S3",
                "AWS RDS",
                "AWS CloudFront",
                "AWS EFS",
                "AWS DynamoDB",
                "CloudWatch",
                "EKS",
                "ECS",
                "MFTS",
                "ALB",
                "NLB",
                "Aurora Postgres",
                "SQS/SNS",
                "Route53",
                "Spring Boot",
                "Restful Microservices",
                "DevOps",
                "Apache Kafka",
                "Splunk",
                "Datadog",
                "Dynatrace",
                "Grafana",
                "Shell scripts",
                "Gradle",
                "Spinnaker"
            ],
            "all": [
                "AWS",
                "Python",
                "Java",
                "SQL",
                "Docker",
                "Kubernetes",
                "Go",
                "Scala",
                "CloudFormation",
                "Terraform",
                "Ansible",
                "GitHub",
                "Jenkins",
                "Maven",
                "Sonar Qube",
                "AWS EC2",
                "AWS S3",
                "AWS RDS",
                "AWS CloudFront",
                "AWS EFS",
                "AWS DynamoDB",
                "CloudWatch",
                "EKS",
                "ECS",
                "MFTS",
                "ALB",
                "NLB",
                "Aurora Postgres",
                "SQS/SNS",
                "Route53",
                "Spring Boot",
                "Restful Microservices",
                "DevOps",
                "Apache Kafka",
                "Splunk",
                "Datadog",
                "Dynatrace",
                "Grafana",
                "Shell scripts",
                "Gradle",
                "Spinnaker"
            ]
        },
        "job_type": [
            "onsite"
        ],
        "contact_person": "Akshat Kumar",
        "email": "akshat@tanishasystems.com",
        "jd": "Greetings..!! My name is Akshat, and I am a Technical Recruiter at Tanisha Systems Inc. Tanisha Systems Inc is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients. Role: AWS PySpark With PythonLocation : Plano TX / Wilmington DE (Onsite- 3 days per week)Type: Contract PositionExperience: 10+Years Job Description:Job responsibilities:Your experience in public cloud migrations of complex systems, anticipating problems, and finding ways to mitigate risk, will be key in leading numerous public cloud initiativesExecutes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problemsOwn end-to-end platform issues & help provide solutions to platform build and performance issues on the AWS Cloud & ensure the deliverables are bug freeDrive, support, and deliver on a strategy to build broad use of Amazon's utility computing web services (e.g., AWS EC2, AWS S3, AWS RDS, AWS CloudFront, AWS EFS, AWS DynamoDB, CloudWatch, EKS, ECS, MFTS, ALB, NLB)Design resilient, secure, and high performing platforms in Public Cloud using best practicesMeasure and optimize system performance, with an eye toward pushing our capabilities forward, getting ahead of customer needs, and innovating to continually improveProvide primary operational support and engineering for the public cloud platform and debug and optimize systems and automate routine tasksCollaborate with a cross-functional team to develop real-world solutions and positive user experiences at every interactionDrive Game days, Resiliency tests and Chaos engineering exercisesUtilize programming languages like Java, Python, SQL, Node, Go, and Scala, Open-Source RDBMS and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of AWS tools and servicesRequired qualifications, capabilities, and skills:Formal training or certification on software engineering concepts and 10+ years applied experienceHands-on practical experience delivering system design, application development, testing, and operational stabilityAdvanced in one or more programming language(s) - Java, Python, GoA strong understanding of business technology drivers and their impact on architecture design, performance and monitoring, best practicesDesign and building web environments on AWS, which includes working with services like EC2, ALB, NLB, Aurora Postgres, DynamoDB, EKS, ECS fargate, MFTS, SQS/SNS, S3 and Route53Advanced in modern technologies such as: Java version 8+, Spring Boot, Restful Microservices, AWS or Cloud Foundry, Kubernetes.Experience using DevOps tools in a cloud environment, such as Ansible, Artifactory, Docker, GitHub, Jenkins, Kubernetes, Maven, and Sonar QubeExperience and knowledge of writing Infrastructure-as-Code (IaC) and Environment-as-Code (EaC), using tools like CloudFormation or TerraformExperience with high volume, SLA critical applications, and building upon messaging and or event-driven architecturesDeep understanding of financial industry and their IT systemsPreferred qualifications, capabilities, and skills:Expert in one or more programming language(s) preferably JavaAWS Associate level certification in Developer, Solutions Architect or DevOpsExperience in building the AWS infrastructure like EKS, EC2, ECS, S3, DynamoDB, RDS, MFTS, Route53, ALB, NLBExperience with high volume, mission critical applications, and building upon messaging and or event-driven architectures using Apache KafkaExperience with logging, observability and monitoring tools including Splunk, Datadog, Dynatrace. CloudWatch or GrafanaExperience in automation and continuous delivery methods using Shell scripts, Gradle, Maven, Jenkins, SpinnakerExperience with microservices architecture, high volume, SLA critical applications and their interdependencies with other applications, microservices and databasesExperience developing process, tooling, and methods to help improve operational maturity RegardsAkshat KumarAkshat@tanishasystems.comwww.tanishasystems.comlinkedin.com/in/akshat-kumar-7245821a299 Wood Ave South, Suite # 308, Iselin, New Jersey, NJ 08830, USA",
        "source": "Email",
        "date_posted": "2024-10-15 07:30:00-05:00",
        "unique_id": "1929048b1b7b74a9",
        "emp_type": [
            "contract",
            "third party"
        ],
        "tag": "AWS Data Engineer"
    }
]