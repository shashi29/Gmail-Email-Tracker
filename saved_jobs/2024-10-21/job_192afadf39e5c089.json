[
    {
        "company": "SPAR Information Systems",
        "job_title": "GCP Data Engineer",
        "location": "Bentonville, AR, USA",
        "full_location": {
            "city": "Bentonville",
            "state": "AR",
            "country": "USA"
        },
        "job_details": {
            "employment_type": [
                "contract",
                "third party"
            ],
            "experience_required": "6+ years",
            "degree_required": "Bachelor\u2019s degree in Computer Science, Computer Engineering or a software related discipline. A Master\u2019s degree in a related field is an added plus",
            "visa_sponsorship": "Not specified",
            "notice_period": "Not specified",
            "duration": "12+ Months",
            "rate": "Not specified"
        },
        "skills": {
            "core": [
                "Hadoop",
                "Hive",
                "Spark",
                "Scala",
                "GCP"
            ],
            "primary": [
                "SQL",
                "Data Warehouse",
                "Agile",
                "Kubernetes",
                "Data Analysis",
                "Data Cleaning",
                "Data Validation",
                "Data Migration",
                "Data Mining",
                "ETL"
            ],
            "secondary": [
                "Apache Hadoop Clusters",
                "Map Reduce",
                "Pig",
                "Airflow",
                "Cloud Administration",
                "PySpark",
                "Python",
                "Data Analytics",
                "Machine Learning",
                "Java",
                "Reactjs",
                "NodeJs"
            ],
            "all": [
                "Hadoop",
                "Hive",
                "Spark",
                "Scala",
                "GCP",
                "SQL",
                "Data Warehouse",
                "Agile",
                "Kubernetes",
                "Data Analysis",
                "Data Cleaning",
                "Data Validation",
                "Data Migration",
                "Data Mining",
                "ETL",
                "Apache Hadoop Clusters",
                "Map Reduce",
                "Pig",
                "Airflow",
                "Cloud Administration",
                "PySpark",
                "Python",
                "Data Analytics",
                "Machine Learning",
                "Java",
                "Reactjs",
                "NodeJs"
            ]
        },
        "job_type": [
            "remote"
        ],
        "contact_person": "Rahul Kumar",
        "email": "rahul.k@sparinfosys.com",
        "jd": "Hello Folks, Must have strong exp in Hadoop, Hive, Spark, Scala, GCP skills AND Need local to Arkansas or if not local then willing to go Onsite - Note itHope you all are doing good.Please go through the Job description and let me know your interest. Title: GCP Data EngineerLocation: Bentonville, AR (Can be Remote)Duration: 12+ Months contractYou bring:\u2022 Bachelor\u2019s degree in Computer Science, Computer Engineering or a software related discipline. A Master\u2019s degree in a related field is an added plus\u2022 6+ years of experience in Data Warehouse and Hadoop/Big Data\u2022 3+ years of experience in strategic data planning, standards, procedures, and governance\u2022 4+ years of hands-on experience in Scala\u2022 4+ years of experience in writing and tuning SQLs, Spark queries\u2022 3+ years of experience working as a member of an Agile team\u2022 Experience with Kubernetes and containers is a plus\u2022 Experience in understanding and managing Hadoop Log Files.\u2022 Experience in understanding Hadoop multiple data processing engines such as interactive SQL, real time streaming, data science and batch processing to handle data stored in a single platform in Yarn.\u2022 Experience in Data Analysis, Data Cleaning (Scrubbing), Data Validation and Verification, Data Conversion, Data Migrations and Data Mining.\u2022 Experience in all the phases of Data warehouse life cycle involving Requirement Analysis, Design, Coding, Testing, and Deployment., ETL Flow\u2022 Experience in architecting, designing, installation, configuration and management of Apache Hadoop Clusters\u2022 Experience in analyzing data in HDFS through Map Reduce, Hive and Pig is a plus\u2022 Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.\u2022 Strong analytic skills related to working with unstructured datasets\u2022 Experience in Migrating Big Data Workloads\u2022 Experience with data pipeline and workflow management tools: Airflow\u2022 Cloud AdministrationAs a Senior Data Engineer this is your opportunity to:\u2022 Assembling large to complex sets of data that meet non-functional and functional business requirements\u2022 Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\u2022 Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using GCP/Azure and SQL technologies\u2022 Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition\u2022 Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues\u2022 Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues\u2022 Strong background in data warehouse design\u2022 Overseeing the integration of new technologies and initiatives into data standards and structures\u2022 Strong Knowledge in Scala, Spark, PySpark, Python, SQL\u2022 Experience in Cloud platform (GCP/Azure) data migration \u2013 Source/Sink mapping, Build pipelines, work flow implementation, ETL and data validation processing\u2022 Strong verbal and written communication skills to effectively share findings with shareholders\u2022 Experience in Data Analytics, optimization, machine learning techniques is added advantage\u2022 Understanding of web-based application development tech stacks like Java, Reactjs, NodeJs is a plus\u00b7Key Responsibilities\u2022 20% Requirements and design\u2022 60% coding & testing and 10% review coding done by developers, analyze and help to solve problems\u2022 10% deployments and release planning For this role, we value:\u2022 The ability to adapt quickly to a fast-paced environment\u2022 Excellent written and oral communication skills\u2022 A critical thinker that challenges assumptions and seeks new ideas\u2022 Proactive sharing of accomplishments, knowledge, lessons, and updates across the organization\u2022 Experience designing, building, testing and releasing software solutions in a complex, large organization\u2022 Demonstrated functional and technical leadership\u2022 Demonstrated analytical and problem-solving skills (ability to identify, formulate, and solve engineering problems) Overall Experience level:8+ years in IT with min 6+ years of Data Engineering and Analyst experience.",
        "source": "Email",
        "date_posted": "2024-10-21 09:43:46-05:00",
        "unique_id": "192afadf39e5c089",
        "emp_type": [
            "contract",
            "third party"
        ],
        "tag": "GCP Data Engineer"
    }
]